Tutorial
==================================

This tutorial will guide you through the use of the transfer learning toolkit, specifically focusing on how to use the transferability metrics provided by the toolkit.

Prerequisites
-------------

Before using the transferability metrics, ensure that you have the following:

- A pre-trained source model.
- A target dataset.
- Feature embeddings of the target dataset, extracted using the source model.
- Predictions (``predictz``) of the target dataset, generated by the source model.

Installation
------------

.. code-block:: bash

    pip install your-toolkit-package-name

Metrics
-------

The toolkit provides five different metrics to assess the transferability of a pre-trained model to a new target task.

Summary of Metric Selection Based on Resources
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

When selecting a metric from the transfer learning toolkit, consider the data and resources available to you:

- If you **only have target feature embeddings** (no labels or source model predictions), then **H-Score** and **LogME** are your go-to metrics.

- If you **have predictions from a source model on the target dataset** but no access to true labels, **LEEP** is appropriate.

- If you **have both source and target feature embeddings** but no labels or predictions, you can use **OTCE** for domain adaptation analysis.

- If you **have source feature embeddings and target predictions**, **NCE** is a suitable choice.

To put it simply:

- **Feature embeddings only (target)**: H-Score, LogME
- **Predictions on target (from source model)**: LEEP
- **Feature embeddings (source and target)**: OTCE
- **Feature embeddings (source) + Predictions on target**: NCE

Remember to always align your metric choice with the specific question you want to answer about transferability and the nature of your source and target datasets.

H-Score
^^^^^^^

The H-score evaluates how well the feature representations from the source model can separate classes in the target dataset.

.. code-block:: python

    h_score(target_root_dir)

- ``target_root_dir`` (str): Directory containing the target dataset feature embeddings.

LEEP
^^^^

LEEP computes the log-likelihood of the true target labels given the pseudo-labels generated by the source model.

.. code-block:: python

    log_expected_empirical_prediction(target_predictz_dir)

- ``target_predictz_dir`` (str): Directory containing the predictions (``predictz``) of the target dataset.

LogME
^^^^^

LogME measures the maximum evidence of the target labels given the feature representations from the source model.

.. code-block:: python

    log_maximum_evidence(target_root_dir)

- ``target_root_dir`` (str): Directory containing the target dataset feature embeddings.

NCE (Negative Conditional Entropy)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

NCE assesses the difficulty of transferring from the source model to the target task by evaluating the conditional entropy.

.. code-block:: python

    negative_conditional_entropy(source_root_dir, target_predictz_dir)

- ``source_root_dir`` (str): Directory containing the source dataset feature embeddings.
- ``target_predictz_dir`` (str): Directory containing the predictions (``predictz``) of the target dataset.

OTCE (Optimal Transport Cost Entropy)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

OTCE quantifies the domain and task difference between source and target datasets using the Optimal Transport problem.

.. code-block:: python

    optimal_transport(source_root_dir, target_root_dir)

- ``source_root_dir`` (str): Directory containing the source dataset feature embeddings.
- ``target_root_dir`` (str): Directory containing the target dataset feature embeddings.

Usage Example
-------------

.. code-block:: python

    from tool.metric import *

    if __name__ == '__main__':
        # Define a list of directory sets
        directory_sets = [
            # Directory set 1
            # Directory set 2
            # Add more directory sets here
        ]

        # Initialize a dictionary to hold the results
        results = {}

        # Iterate over each directory set and calculate metrics
        for i, dirs in enumerate(directory_sets):
            # Calculate each metric and store the result
            results[i] = {
                'h_score': h_score(dirs['tar_root_dir']),
                # Other metrics
            }

        # Print the results
        for i, result in results.items():
            print(f"Results for directory set {i}:")
            for metric, value in result.items():
                print(f"{metric}: {value}")

Conclusion
----------

This tutorial provided an overview of how to use the transferability metrics in the toolkit. By following the instructions, you can evaluate different pre-trained models and determine their suitability for your target tasks.

